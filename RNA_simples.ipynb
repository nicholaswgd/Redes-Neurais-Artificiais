{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Introdução\n",
    "\n",
    "Antes de começarmos, preciso garantir que possamos conversar na mesma língua. Por isso, peço que leia a [introdução ao aprendizado de máquina](https://matheusfacure.github.io/AM-Essencial/) que fiz especialmente para quem quer começar a entender esse assunto. Lá, falo sobre o que é e para que serve aprendizado de máquina e apresento alguns do principais conceitos dessa ciência, como os três tipos de aprendizado, o dilema de viés e variância, treinamento, avaliação e validação cruzada. Reconheço que é um post um pouco longo, mas, por favor, invista um pouco do seu tempo nele e entenda bem os conceitos lá apresentados. \n",
    "\n",
    "Bom, espero que você tenha lido o que recomendei. A partir desse ponto, vou usar alguns dos termos que apresentei lá (e que, talvez, você tenha acabado de aprender). Isso me permitirá tornar este post mais curto, já que não precisarei explicar todos os conceitos de aprendizado de máquina do zero.\n",
    "\n",
    "**Aviso:** Ao final deste post, você entenderá intuitivamente o que é uma rede neural e será capaz de treinar uma para reconhecimento de imagem. Mesmo assim, esse post é apenas uma breve introdução ao aprendizado de máquina contemporâneo. Pensei nele como uma forma de instigar a sua curiosidade e lhe convencer de que aprendizado de máquina é um assunto no qual vale a pena se aprofundar. Assim, do **fundo do meu coração**, não terminem essa leitura achando que já sabem suficiente e, por favor, continuem aprendendo mais, bem mais, do que o conteúdo apresentado aqui.\n",
    "\n",
    "## Aprendizado de Máquina Contemporâneo\n",
    "\n",
    "A maioria dos algoritmos de aprendizado de máquina são do século passado. Alguns, como redes neurais, são especialmente velhos, tendo sido inventados na década de 50. Por que, então, só agora é que estamos vendo aprendizado de máquina aparecendo em todos os cantos e sendo anunciado pelas revistas e jornais aos quatro ventos? Podemos argumentar que, embora os algoritmos em si sejam velhos, algumas pequenas melhorias mais recentes os tornaram, finalmente, extremamente úteis. Isso é parcialmente verdade, mas foram outros dois fatores que mais contribuíram para o atual renascimento da inteligência artificial:  \n",
    "\n",
    "    1. Aumento do poder computacional (leia-se GPUs enormes)\n",
    "    2. Aumento da disponibilidade de dados (leia-se *Big Data* ou simplesmente bases de dados maiores)\n",
    "\n",
    "Infelizmente, ainda hoje, para que um modelo de aprendizado de máquina consiga extrapolar um padrão aprendido, ele precisa de uma abundância de exemplos desses padrões. Por exemplo, para que um sistema inteligente consiga interpretar uma palavra, ele precisa antes ver milhões de frases para entender como as plavras se relacionam. Isso só foi possível recentemente, com expansão da quantidade de dados e da capacidade computacional para processá-los. \n",
    "\n",
    "Devemos ter em mente que os **sistemas de inteligência artificias contemporâneos não são nada mais do que modelos matemáticos complexos que conseguem aprender uma representação simplificada da realidade, a partir da extração de padrões estatísticos presentes nos dados, que são, por sua vez, extraídos dessa mesma realidade que motiva o aprendizado**. E, [por enquanto](https://arxiv.org/abs/1605.06065), um ponto fraco desses sistemas ou modelos estatísticos é que eles precisam de muitos dados para conseguir entender os padrões que se apresentam na nossa complexa realidade.\n",
    "\n",
    "Mas como exatamente esses sistemas conseguem entender a nossa realidade? Infelizmente, ainda não podemos dizer com certeza como isso é feito. Vou então dar uma explicação que é a mais aceita e cujas evidenciais são mais fortes. Em poucas palavras, os sistemas de inteligência artificial modernos primeiro aprendem uma **representação interna abstrata** dos dados brutos e, a partir dessa representação abstrata, realizam alguma tarefa, geralmente uma previsão. Por exemplo, considere a tarefa de prever que objeto está em uma imagem. Nesse caso, os dados são simples pixeis com quantidades de vermelho, verde e azul (se a imagem for preta e branca, os pixeis são ainda mais simples, indicando apenas a quantidade de preto). Como é muito difícil sair desses dados brutos para conceitos abstratos - como a transparência de uma garrafa pet, um olho em uma face, o telhado de uma casa -, o sistema antes converte os pixeis de uma imagem em algo mais abstrato, como o conceito visual de um olho. \n",
    "\n",
    "Isso é provavelmente o que eu e você fazemos quando enxergamos. O nosso corpo percebe raios multicoloridos refletidos nos objetos, mas o que vemos são conceitos com uma carga semântica muito maior, tais como a textura da madeira em uma cadeira ou a silhueta opaca de um gato branco passeando sob a meia luz. Para representar esse nível de abstração, os sistemas contemporâneos de IA são geralmente construídos em camadas. Podemos pensar nelas como **níveis hierárquicos de abstração** que serão aprendidos por um modelo estatístico. Por exemplo, a primeira camada de um sistema pode aprende a abstrair pixeis em cantos e quinas de objetos ou diferenças de contraste e luminosidade; a segunda camada, partido das abstrações da primeira, converte os cantos e quina em formas mais elaborados, como círculos, triângulos e quadrados; a terceira camada então pode partir dessas formas para criar abstrações sobre parte de objetos, como a roda de um carro ou o bico de um papagaio; por fim, o sistema usa essas abstrações finais para identificar o que está em uma foto colorida. (Isso é mais do que um mero exemplo. Na verdade, existem [formas um tanto divertidas de ver as abstrações aprendidas por IAs](https://matheusfacure.github.io/2017/05/09/deepdream/)).\n",
    "\n",
    " Esse modo de estruturar os sistemas de IA é o que leva o nome de **Deep Learning** (aprendizado profundo) ou aprendizado de representações. A palavra \"profundo\" vem do simples fato de construímos nossos sistemas empilhando camadas.\n",
    "\n",
    "## Neurônios\n",
    "\n",
    "Nesta introdução, para exemplificar a construção e o treinamento de um sistema moderno de IA, vamos realizar uma simples tarefa de visão computacional, na qual usaremos uma rede neural bem simples para reconhecer dígitos escritos. Em termos técnicos, será uma tarefa de [OCR (Optical Character Recognition)](https://pt.wikipedia.org/wiki/Reconhecimento_%C3%B3tico_de_caracteres). Mas, antes de entendermos o que são e como treinar redes neurais, precisamos falar sobre seu componente mais básico: os neurônios. \n",
    "\n",
    "![neurônio](https://matheusfacure.github.io/img/perceptron.png)\n",
    "\n",
    "Como grande parte dos algoritmos de aprendizado de máquina, os neurônios são modelos matemáticos (ou funções) que representam a realidade de forma simplificada. Eles são compostos por uma soma ponderada, seguida ou não de uma função ativação. Por exemplo, considere a tarefa de prever se o preço de uma casa será maior ou menor do que a média, dadas as variáveis \\\\(x_1\\\\), o tamanho da casa em metros quadrados, \\\\(x_2\\\\), o índice de pobreza vizinhança e \\\\(x_3\\\\) o tamanho do meu cabelo. Podemos facilmente utilizar um neurônio para resolver essa tarefa. Note que, provavelmente, quanto maior \\\\(x_1\\\\), maior a probabilidade da casa ter um preço acima da média (e vice versa). Assim, devemos esperar que o peso de \\\\(x_1\\\\), \\\\(w_1\\\\), na soma ponderada do nosso neurônio seja positivo, indicando que essa variável tem um impacto igualmente positivo na probabilidade do preço da casa ser acima da média. Com o mesmo raciocínio, podemos argumentar que \\\\(w_2\\\\) será negativo. Note que esses dois pesos não precisam ter a mesma intensidade. Pode ser que o impacto positivo de \\\\(x_1\\\\) seja muito maior que o impacto negativo de \\\\(x_2\\\\), de forma que \\\\(w_1\\\\) seja maior que \\\\(w_2\\\\). Em palavras, pode ser que o tamanho da casa seja um determinante mais importante do preço do que o índice de pobreza da vizinhança. Por fim, é provável que o tamanho do meu cabelo, \\\\(x_3\\\\), não tenha muito impacto no preço de uma casa. Por isso, esperamos que \\\\(w_3\\\\) seja muito próximo de zero na soma ponderada do nosso neurônio. Isso indica que essa variável influencia pouco o preço da casa. Repare também que temos uma variável que é sempre \\\\(1\\\\). A ponderação desse \\\\(1\\\\) com o \\\\(w_0\\\\) é o que chamamos de viés. Esse viés captura a tendência da casa ter valor alto, uma vez que já tenhamos considerado as outras variáveis. Por fim, é importante ressaltar que os \\\\(w\\\\)s são o que chamamos de parâmetros do modelo. Eles são variáveis que o neurônio (e, mais para frente, a rede neural) vai aprender (ou estimar) durante o treinamento.\n",
    "\n",
    "Além da soma ponderada, nosso neurônio precisa de uma função de ativação. Isso porque a soma ponderada pode nos dar um resultado qualquer, mas, como nossa previsão é uma probabilidade, precisamos de uma função ativação que converta um número qualquer, positivo ou negativo, em um valor entre 0 e 1. A função que faz isso chama função [softmax](https://en.wikipedia.org/wiki/Softmax_function), então a usaremos após a soma ponderada do nosso neurônio. Existem várias funções de ativação e, dependendo da tarefa em questão, uma é mais recomendada do que outra. Infelizmente, para falar delas é preciso mais conhecimento matemático. Intuitivamente, quando falarmos delas nas redes neurais, pense na função ativação como algo que dá um comportamento mais complexo aos neurônios. Elas também são fundamentais nas redes neurais, para que essas consigam representar padrões complexos. \n",
    "\n",
    "## Redes Neurais Artificiais\n",
    "\n",
    "Infelizmente, os neurônios são bastante limitados. Em aprendizado de máquina, queremos que um algoritmo possa aprender qualquer tipo de padrão presente nos dados, mas isso não é possível com um simples neurônio. Por isso, construímos as redes neurais, que são simplesmente vários neurônios conectados. Pense nos neurônios como blocos de Lego e nas redes neurais como estruturas que montamos empilhando esses blocos de Lego. Dependendo da tarefa, uma estrutura pode se mais útil do que outra. No entanto, aqui, vamos considerar apenas a estrutura mais simples e mais comun de rede neural, o modelo de **redes neurais *feedforward* densas**.\n",
    "\n",
    "![neurônio](http://www.texample.net/media/tikz/examples/PNG/neural-network.png)\n",
    "\n",
    "Na rede neural acima, como exemplo, podemos dizer que ainda estamos lidando com o problema de prever se o preço de uma casa será acima ou abaixo da média. Na entrada da rede, temos as mesmas 3 variáveis mais o viés, que são representados pelas bolinhas verdes. Isso é o que chamamos de camada de entrada da rede neural. Em seguida, utilizando 5 neurônios, realizamos 5 somas ponderadas seguidas de uma função de ativação. Essas operações são representadas pelas bolinhas azuis, que recebem o nome de camada oculta da rede neural. Por fim, utilizamos um único neurônio que realiza uma soma ponderada do resultado dos neurônios anteriores e então converte essa soma ponderada em uma probabilidade com a função softmax. Isso é o que chamamos de camada de saída da rede neural e está representado pela bolinha vermelha.\n",
    "\n",
    "Ignore a camada de entrada (verde) por um momento. Note como a camada de saída mais a camada oculta é exatamente o modelo de neurônio que vimos antes? A camada de saída é simplesmente um modelo de neurônio, que está tratando a camada oculta como se fosse as variáveis independentes que determinam a variável de resposta (no nosso exemplo, a probabilidade do preço da casa ser alto). Assim, podemos ver que a rede neural está **aprendendo novas variáveis** e usando um modelo de neurônio nessas novas variáveis. Esse é o princípio básico de *deep learning*: aprender variáveis representativas, geralmente mais abstratas, que auxiliem na tarefa em questão, no caso, uma tarefa de previsão.\n",
    "\n",
    "Podemos ir ainda um passo além e adicionar uma segunda camada oculta. \n",
    "\n",
    "![neurônio](http://cs231n.github.io/assets/nn1/neural_net2.jpeg)\n",
    "\n",
    "Isso aumenta ainda mais o poder representativo da rede neural. Lembre-se de que **podemos pensar nas camadas da rede neural como níveis hierárquicos de abstração**.\n",
    "\n",
    "## Treinando RNAs\n",
    "\n",
    "Agora que entendemos o que são redes neurais em um nível intuitivo, precisamos saber como treiná-las. Isso é feito por um processo de otimização no qual minimizamos uma função custo (ou objetivo). Para manter o nível de simplicidade, pense na função custo como algo que mede a diferença entre o que a rede neural prevê o que de fato foi observado. Por exemplo, se a rede neural prever um valor pequeno para a probabilidade de uma casa ter preço acima da média, mas a casa, na verdade, for bastante cara, então a função custo terá um valor alto.\n",
    "\n",
    "Para iniciar o treinamento, vamos chutar alguns valores para os \\\\(w\\\\)s de cada neurônio. Em seguida, vamos ver a previsão da rede neural em alguns dados, que, muito provavelmente, será péssima. Dessa forma, os \\\\(w\\\\)s iniciais serão associados a um alto custo ou a uma **região elevada na superfície de custo**. No treinamento então, vamos atualizar os \\\\(w\\\\)s de maneira iterativa, de forma a diminuir o custo. Isso é feito com a técnica de gradiente descendente estocástico, que pode ser entendida como uma descida na superfície de custo de uma tarefa de otimização.\n",
    "\n",
    "![neurônio](https://sebastianraschka.com/images/blog/2015/singlelayer_neural_networks_files/perceptron_gradient_descent_1.png)\n",
    "\n",
    "Para entender a fundo essa técnica, é preciso saber cálculo multivariado, mas, intuitivamente, ela é bem simples. Em primeiro lugar, pegamos aleatoriamente (daí a palavra estocástico) um pequeno punhado de dados para conseguir uma estimativa da nossa posição na superfície de custo. Então, movemos os parâmetros \\\\(w\\\\) na direção oposta da inclinação dessa superfície. Isso é como dar um passo para baixo na superfície de custo. Com passos suficientes, nossa esperança é que os \\\\(w\\\\) nos coloque em uma região de custo (ou erro) baixa o suficiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Construindo e Treinando uma Rede Neural Artificial\n",
    "\n",
    "\n",
    "Muito bem, tudo isso já está parecendo bastante promissor. Vamos então construir e treinar uma rede neural de duas camadas ocultas para uma a trefa simples de reconhecimento de caracteres em imagens. Para isso, vou usar a linguagem de programação Python, mas não se preocupe, não será preciso saber programar para entender o que vem a seguir. Na verdade, quase não vou usar programação. Peço então que, por hora, você pense no Python apenas como um programa de computador como outro qualquer. Com ele, podemos digitar uma série de comandos para que o computador os execute. Aqui, vou simplesmente mostrar quais os comando necessários para construir e treinar uma simples rede neural.\n",
    "\n",
    "Junto com o Python, vou usar alguns pacotes para ajudar na construção e treinamento de redes neurais. Você pode pensar nesses pacotes como extensões de um programa de computador. Mais concretamente, pense em um pacote do Python como uma fonte nova que você baixa para seu editor de texto. Particularmente importante será o pacote [TensorFlow](https://en.wikipedia.org/wiki/TensorFlow), que foi desenvolvido pelo Google e feito *open source* (aberto) em 2015.\n",
    "\n",
    "Se você ainda não tem esses programas, não se preocupe. Também fiz um [tutorial sobre como instalá-los](https://lamfo-unb.github.io/2017/06/10/Instalando-Python/). Sugiro que você instale os programas necessários e acompanhe o tutorial a seguir executando cada passo no seu computador.\n",
    "\n",
    "Em mais enrolações, no Python, vamos importar alguns pacotes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf # importa o TensorFlow\n",
    "from tensorflow.contrib.learn import DNNClassifier # importa o modelo de rede neural do TensorFlow\n",
    "\n",
    "# pacotes adicionais\n",
    "import numpy as np # para computação numérica menos intensiva\n",
    "import os # para criar pastas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**OBS**: em uma sequência de comandos (código) de Python, o computador ignora tudo que for escrito após #, por isso, # indica o início de um comentário meu explicando o código.\n",
    "\n",
    "### Os dados\n",
    "\n",
    "Em primeiro lugar, vamos ver como são nossos dados. Já disse que trabalharemos com OCR. Em particular, vamos usar a base de dados [MNIST](https://en.wikipedia.org/wiki/MNIST_database), que contém 55 mil dados de treino e 10 mil dados de teste. Os dados são imagens de 28x28 pixeis, o que nos dá 784 variáveis para colocar na camada de entrada da nossa rede neural. Mas, antes disso, vamos criar uma nova pasta no nosso computador e baixar esses dados nela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tmp/train-images-idx3-ubyte.gz\n",
      "Extracting tmp/train-labels-idx1-ubyte.gz\n",
      "Extracting tmp/t10k-images-idx3-ubyte.gz\n",
      "Extracting tmp/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# criamos uma pasta para colocar os dados\n",
    "if not os.path.exists('tmp'): # se a pasta não existir\n",
    "    os.makedirs('tmp') # cria a pasta\n",
    "\n",
    "# baixa os dados na pasta criada \n",
    "from tensorflow.examples.tutorials.mnist import input_data # baixa os dados\n",
    "data = input_data.read_data_sets(\"tmp/\", one_hot=False) # baixa e carrega os dados já formatados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Após executar os comandos acima, todos os dados, tanto de treino como de teste, estão armazenados em `data`. Você pode pensar em `data` como uma planilha do Excel aberta. As únicas diferenças são que você não pode interagir com a linhas e colunas da planilha clicando com o mouse e que você não pode ver a planilha a todo tempo. No entanto, podemos facilmente ver como são nossos dados usando o comando `print()` do Python. Para interagir com os dados em `data`, usamos a notação de ponto `.`. Por exemplo, `data.train.images` nos dá acesso aos dados de treino e, a partir daí, às imagens de treino. Podemos ir além e usar `data.train.images.shape` para ver o atributo de tamanho dos dados de treino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 784)\n",
      "(10000, 784)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(data.train.images.shape) # mostra o formato dos dados de treino\n",
    "print(data.test.images.shape) # mostra o formato dos dados de teste\n",
    "print(data.train.images) # mostra algumas linhas e colunas dos dados de treino"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Acima, podemos ver que os dados de treino são uma tabela com 55 mil linhas, cada uma com uma observação de 784 colunas, (que representam os pixeis de uma imagem). Como nossa tarefa se encaixa no regime de aprendizado supervisionado, cada imagem vem anotada com um alvo, o dígito que está nela (e que queremos prever). Podemos acessar os alvos de treino com o comando `print()` mais `data.train.labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000,)\n",
      "[7 3 4 ..., 5 6 8]\n"
     ]
    }
   ],
   "source": [
    "print(data.train.labels.shape) # mostra o formato dos alvos de treino\n",
    "print(data.train.labels) # mostra os alvos de treino"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As anotações da base de treino podem ser entendidas como uma tabela de 55 mil linhas e com uma única coluna. Pelo que vemos acima, nossa primeira imagem é de um 7, nossa segunda imagem é um 3 e assim por diante. Para ficar menos abstrato, vamos ver como são essas imagens. Isso necessitará de um pacote do Python para ver imagens. Abaixo, importamos esse pacote, desenhamos a segunda imagem de treino com o comando `plt.imshow(...)` (usamos índice 1 para selecionar a segunda imagem, pois em ciência da computação se começa a contar do zero), usamos o alvo correspondente como título da imagem com `plt.title(...)` e, por fim, usamos `plt.show()` para mostras a imagem. Além disso, na imagem, precisamos usar `.reshape(28,28)` para reformatar ela de uma linha com 784 pixeis para uma grade com 28px de altura e largura. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWEAAAFyCAYAAAAkvWviAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niQxhSUpkCEtSov8PixFwVAtbEAcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9a2f3979b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt # importa pacote para mostrar imagens\n",
    "\n",
    "# mostra a primeira imagem no set de treino\n",
    "plt.imshow(data.train.images[1].reshape(28,28), cmap='Greys', interpolation='nearest')\n",
    "plt.title(str(data.train.labels[1])) # anotação do dígito\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Definindo os hiper-parâmetros\n",
    "Tudo parece OK com os nossos dados. Podemos então começar a construção da rede neural. O primeiro passo é definir os hiper-parâmetros do modelo. Diferentemente dos parâmetros da rede, os \\\\(w\\\\), os hiper-parâmetros não são naturalmente aprendidos durante o treinamento e devem ser ajustados à mão. Alguns dos hiper-parâmetros mais importantes da rede neural são o número de camadas e o número de neurônios em cada camada. Esses hiper-parâmetros definem a capacidade da rede neural e, por meio deles, podemos ajustar o [*trade-off* entre erro por viés e por variância](https://matheusfacure.github.io/AM-Essencial/#Viés-e-variância). Quanto maior o número de neurônios, mais potente será a rede neural, mas maior será a probabilidade dela sofrer com sobre-ajustamento.\n",
    "\n",
    "Outros hiper-parâmetros da rede neural são o tamanho do punhado de dados usado durante a otimização e o tamanho do passo dado a cada iteração de treino. Em outras palavras, o tamanho do punhado de dados define quão precisa será nossa estimativa local da superfície de custo, enquanto que a taxa de aprendizado definirá o tamanho do passo em cada descida nessa superfície de custo.\n",
    "\n",
    "Outro detalhe importante é que a rede neural que vamos construir não tem apenas um neurônio na camada de saída, mas 10 neurônios. Casa neurônios representará a probabilidade da imagem conter um dos dígitos de 0 a 9. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# definindo constantes\n",
    "lr = 0.01 # taxa de aprendizado\n",
    "n_iter = 1000 # número de iterações de treino\n",
    "batch_size = 128 # qtd. de imagens no punhado de dados\n",
    "n_inputs = 28 * 28 # número de variáveis (pixeis)\n",
    "n_l1 = 512 # número de neurônios da primeira camada\n",
    "n_l2 = 512 # número de neurônios da segunda camada\n",
    "n_outputs = 10 # número de neurônios da camada de saída"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Nossa rede neural terá duas camadas, cada uma com 512 neurônios. \n",
    "\n",
    "### Construindo a rede neural\n",
    "\n",
    "Felizmente, não precisamos construir uma rede neural do zero. Como elas são extremamente populares, outras pessoas já as deixaram pré-montada para facilitar a nossa vida. A única coisa que precisamos fazer é dizer quais serão os hiper-parâmetros da rede neural que o pacote TensorFlow tomará conta de construí-la para nós.\n",
    "\n",
    "O primeiro passo é converter os dados para um formato com que TensorFlow consiga trabalhar facilmente. Isso é feito com o comando `tf.contrib.learn.infer_real_valued_columns_from_input(...)` e passamos como argumento desse comando as imagens de treino. Por fim, passamos os hiper-parâmetros definidos acima para o comando `DNNClassifier(...)`, o que cria a nossa rede neural e a armazena em `deep_ann`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f9a2d858ba8>, '_task_id': 0, '_model_dir': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_num_worker_replicas': 0, '_save_checkpoints_steps': None, '_master': '', '_environment': 'local', '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_task_type': None, '_save_summary_steps': 100, '_keep_checkpoint_max': 5, '_is_chief': True}\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp1trkko6g\n"
     ]
    }
   ],
   "source": [
    "# converte os dados\n",
    "x_input = tf.contrib.learn.infer_real_valued_columns_from_input(data.train.images)\n",
    "\n",
    "# cria a rede neural\n",
    "deep_ann = DNNClassifier(hidden_units = [n_l1, n_l2], # qtd. de neurônios por camada\n",
    "                        feature_columns = x_input, # camada de entrada (dados)\n",
    "                        n_classes = n_outputs, # número de classes (10 dígitos)\n",
    "                        activation_fn = tf.nn.relu, # função de ativação das camadas\n",
    "                        optimizer = tf.train.AdamOptimizer(learning_rate=lr)) # otimizador "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Treinando e Avaliando a RNA\n",
    "\n",
    "Para treinar a rede neural criada acima basta um único comando do TensorFlow. Esse comando vem com o nosso modelo de rede neural e podemos acessá-lo com a notação de ponto a partir da rede neural criada acima: `deep_ann.fit(...)`. Passamos ao comando as imagens de treino com os respectivos dígitos anotados. Lembre-se de que esse é um problema de aprendizado de máquina supervisionado e de que a tarefa da rede neural é aprender como mapear dos valores numéricos dos pixeis no dígito que está escrito na imagem. Por isso, precisamos passar também as anotações das imagens de treino: `data.train.labels`. Antes de passar esses dados à rede neural, precisamos convertê-los para os tipos aceitos pelo modelo. As imagens devem ser do tipo `float32` (dígitos com casas decimais), enquanto que as anotações devem ser do tipo `int64` (dígitos inteiros, sem casas decimais).  Por fim, passamos para o comando de treinamento o número de iterações de treino (passos na caminhada para baixo na superfície de custo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-9d6ce0d5b84b>:4: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with batch_size is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From <ipython-input-8-9d6ce0d5b84b>:4: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From <ipython-input-8-9d6ce0d5b84b>:4: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:615: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py:248: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  equality = a == b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmp1trkko6g/model.ckpt.\n",
      "INFO:tensorflow:loss = 2.37428, step = 1\n",
      "INFO:tensorflow:global_step/sec: 47.2305\n",
      "INFO:tensorflow:loss = 0.143084, step = 101 (2.118 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.1761\n",
      "INFO:tensorflow:loss = 0.120638, step = 201 (2.120 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.4831\n",
      "INFO:tensorflow:loss = 0.255165, step = 301 (2.106 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.3015\n",
      "INFO:tensorflow:loss = 0.26809, step = 401 (2.114 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.6499\n",
      "INFO:tensorflow:loss = 0.206926, step = 501 (2.144 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.2371\n",
      "INFO:tensorflow:loss = 0.0891523, step = 601 (2.117 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.2558\n",
      "INFO:tensorflow:loss = 0.0645118, step = 701 (2.116 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.5285\n",
      "INFO:tensorflow:loss = 0.148939, step = 801 (2.104 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.1638\n",
      "INFO:tensorflow:loss = 0.0710656, step = 901 (2.120 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into /tmp/tmp1trkko6g/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0835194.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DNNClassifier(params={'head': <tensorflow.contrib.learn.python.learn.estimators.head._MultiClassHead object at 0x7f9a2d8885f8>, 'optimizer': <tensorflow.python.training.adam.AdamOptimizer object at 0x7f9a2d888278>, 'hidden_units': [512, 512], 'gradient_clip_norm': None, 'activation_fn': <function relu at 0x7f9a437d9ae8>, 'dropout': None, 'embedding_lr_multipliers': None, 'feature_columns': (_RealValuedColumn(column_name='', dimension=784, default_value=None, dtype=tf.float32, normalizer=None),), 'input_layer_min_slice_size': None})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deep_ann.fit(x=data.train.images.astype(np.float32), # conversão de tipo\n",
    "            y=data.train.labels.astype(np.int64), # conversão de tipo\n",
    "            batch_size=batch_size, # tamanho do punhado de dados\n",
    "            steps=n_iter) # iterações de treino"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Após treinada, precisamos avaliar nossa rede neural nos dados de teste. Para isso, usamos o comando `.evaluate`, que pode ser acessado uma vez que a rede neural for treinada. Passamos os dados e anotações de teste para esse comando e podemos ver a acurácia da rede neural, isto é, a taxa de acerto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-ad4f09dfe61b>:2: calling BaseEstimator.evaluate (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From <ipython-input-9-ad4f09dfe61b>:2: calling BaseEstimator.evaluate (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:615: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Starting evaluation at 2017-06-18-14:21:24\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmp1trkko6g/model.ckpt-1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py:248: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  equality = a == b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished evaluation at 2017-06-18-14:21:25\n",
      "INFO:tensorflow:Saving dict for global step 1000: accuracy = 0.9603, global_step = 1000, loss = 0.145026\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.96030003, 'global_step': 1000, 'loss': 0.14502561}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deep_ann.evaluate(data.test.images.astype(np.float32), # variáveis independentes\n",
    "                  data.test.labels.astype(np.int64)) # variáveis dependentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Com apenas 1000 iterações de treino, nossa simples rede neural já consegue uma taxa de acerto de 96%. Isso não é um resultado muito bom, mas já é satisfatório, principalmente se considerarmos quão simples foi treinar esse modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Próximos Passos\n",
    "Como já disse, este post é bastante introdutório e tem o propósito de simplesmente instigar a curiosidade sobre aprendizado de máquina. Dentre as coisas que não dá para falar em um post introdutório estão outras arquiteturas de redes neurais (convolucionais, recorrentes, *autoencoders*, *deep-q*, adversárias...), formas de acelerar o treinamento, técnicas de regularização para controlar o sobre-ajustamento, isso sem falar nas outras classes de modelos de aprendizado de máquina, como máquinas de suporte vetoriais, árvores de decisão, Bayes ingênuo, k-vizinhos mais próximos...\n",
    "\n",
    "Além disso, mesmo em se tratando de redes neurais bem simples como a que treinamos, o nível de abstração desse post é demasiadamente elevado, isto é, não mostrei como construir cada camada de neurônios, como conectá-las nem como construir o algoritmo de treinamento. Embora você possa utilizar aprendizado de máquina sem se preocupar muito com esses detalhes mais mecânicos, conhecê-los é extremamente importante. Entender de fato o funcionamento dos modelos é condição fundamental para solucionar possíveis falhas no treinamento, para descobrir quais modelos utilizar em cada situação e até mesmo para criar novos modelos ou melhorias que possam contribuir para a ciência de aprendizado de máquina. \n",
    "\n",
    "Termino com um apelo: não seja um cientista de dados que simplesmente ficam tentando aplicar um modelo caixa preta atrás do outro, na esperança de que algum resolva seu problema. **Isso não é aprendizado de máquina; é tentativa e erro**. Se for estudar aprendizado de máquina, entenda **BEM** as ferramentas que você usa. Entenda a matemática e a mecânica de um algoritmo em seus mínimos detalhes. Se possível, implemente os modelos de aprendizado de máquina sem o auxílio de pacotes. E, por fim, detalhes são muito, muito, muito importantes! São a eles que devemos parte do renascimento atual de IA, então não os deixe passar batido.\n",
    "\n",
    "Não é difícil encontrar material gratuito e de qualidade na internet, mas também é fácil cair em armadilhas, aprendendo apenas IA em um nível intuitivo, sem se adentrar nos detalhes. Aqui vai então uma pequena lista de onde encontrar conteúdo de qualidade sobre aprendizado de máquina:\n",
    "\n",
    "<ul>\n",
    "\t<li>Blogs\n",
    "<ul>\n",
    "\t<li><a href=\"https://matheusfacure.github.io/tutorials/\">{Quinhentos:Nove}</a>: Sou suspeito para falar, mas acredito que meu blog tenha os melhores tutoriais em português sobre aprendizado de máquina. Em parte, eu o criei com o objetivo de corrigir algumas falhas que percebi nos outros blogs sobre AM. Percebi que o conteúdo deles ou eram simplesmente intuitivos, sem aprofundamento na matemática e na implementação dos modelos, ou eram muito técnicos. Por isso, comecei a criar tutoriais estruturados para conter uma explicação intuitiva da técnica explicada, seguida de uma explicação matemática mais aprofundada e terminando com uma implementação bem documentada. Além disso, eu frequentemente <a href=\"https://matheusfacure.github.io/blog/\">posto sobre meus trabalhos </a> para que sirvam como exemplos de aplicações de aprendizado de máquina.</li>\n",
    "\t<li><a href=\"https://lamfo-unb.github.io/\">Blog do LAMFO</a>: O nosso blog é uma excelente fonte tutoriais e exemplos de aplicações de aprendizado de máquina. O LAMFO é a primeira entidade acadêmica no Brasil a tratar de aprendizado de máquina dentro da grande área de ciências humanas (e até onde sei, ainda é a única que atua na área), então acho que posso dizer que somos autoridade nesse assunto.</li>\n",
    "\t<li><a href=\"http://colah.github.io/\">colah's blog</a>: Esse é um dos melhores blogs que conheço sobre aprendizado de máquina. Infelizmente, o conteúdo é mais avançado e com pouco enfoque em questões práticas (aquele problema do qual falei sobre ser muito técnico).</li>\n",
    "    <li><a href=\"https://r2rt.com/\">R2RT</a>: É um blog excelente, tanto em termos de tutoriais quanto em termos de explicação técnica e matemática por detrás dos algoritmos ensinados. Infelizmente, já começa no nível avançado, sem uma progressão clara entre os tutoriais.</li>\n",
    "    <li><a href=\"http://karpathy.github.io/\">Andrej Karpathy blog</a>: Andrej Karpathy é um pesquisador extremamente inteligente e que escreve com uma simplicidade incrível para alguém do seu nível de conhecimento. Esse não é um blog sobre tutoriais, mas fala sobre assuntos muito interessantes em aprendizado de máquina, além de fornecer implementações detalhadas e bem documentadas sobre o assunto tratado.</li>\n",
    "        \n",
    "</ul>\n",
    "</li>\n",
    "\t<li>Livros\n",
    "<ul> \n",
    "    <li><a href=\"http://www.deeplearningbook.org/\">Deep Learning (Goodfellow et al, 2016)</a>: Este é o melhor livro de aprendizado de máquina que já li. Em um nível teórico, ele trata tanto das práticas modernas de aprendizado de máquina quanto das fronteiras de pesquisa dessa ciência. Assim, se faz essencial tanto para engenheiros que querem apenas utilizar técnicas já bem estabelecidas quanto para pesquisadores que estão interessados em contribuir para o estado da arte.</li>\n",
    "    <li><a href=\"http://shop.oreilly.com/product/0636920052289.do\">Hands-On Machine Learning with Scikit-Learn and TensorFlow (Aurélien Géron, 2017)</a>: Este é o melhor livro prático de aprendizado de máquina que já li. Recomendo como um segundo livro, para ser lido após ter entendido bem a teoria e matemática de aprendizado de máquina.\n",
    "    </li>\n",
    "    <li><a href=\"http://shop.oreilly.com/product/0636920052289.do\">Introduction to Machine Learning (Ethem Alpaydin, 2009)</a>: Um ótimo livro introdutório, para quem quer começar a estudar aprendizado de máquina.\n",
    "    </li>\n",
    "</ul>\n",
    "</li>\n",
    "</ul>\n",
    "\n",
    "## Referências\n",
    "\n",
    "Esse post abrange o conteúdo que dei em um *workshop* do LAMFO sobre redes neurais. Infelizmente, como as imagens da apresentação não estão com as devidas referências (por falta de espaço no slide), compartilhá-la indiscriminadamente pode se tornar problemática para mim. No entanto, caso queira muito ter acesso à apresentação, me [mande um e-mail](mailto:matheusfacure01@gmail.com) explicando por que a quer que nós do LAMFO veremos o que podemos fazer por você.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Exercício\n",
    "Crie uma rede neural para prever se uma pessoa tem renda acima de 50 mil. Os comandos abaixo baixam e formatam os dados. Não se preocupe em entender como isso é feito e foque apenas na construção da rede neural, que pode ser feita de maneira análoga a que vimos acima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21815, 14)\n",
      "(10746, 14)\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# cria uma pasta temporária para salvar os dados\n",
    "train_file = tempfile.NamedTemporaryFile() \n",
    "\n",
    "# baixa os dados\n",
    "urllib.request.urlretrieve(\"http://mlr.cs.umass.edu/ml/machine-learning-databases/adult/adult.data\", train_file.name)\n",
    "\n",
    "# nome das colunas\n",
    "COLUMNS = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\",\n",
    "           \"marital_status\", \"occupation\", \"relationship\", \"race\", \"gender\",\n",
    "           \"capital_gain\", \"capital_loss\", \"hours_per_week\", \"native_country\",\n",
    "           \"income_bracket\"]\n",
    "\n",
    "# le os dados\n",
    "df = pd.read_csv(train_file, names=COLUMNS, skipinitialspace=True)\n",
    "\n",
    "# cria os targets\n",
    "y = (df['income_bracket'].values == '<=50K').astype(int)\n",
    "\n",
    "# retira os targets dos dados\n",
    "df.drop('income_bracket', axis=1, inplace=True)\n",
    "\n",
    "# Escalona as variáveis numéricas\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "numerical = ['age', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week']\n",
    "df[numerical] = scaler.fit_transform(df[numerical])\n",
    "X = df.values\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# importe os pacotes necessários\n",
    "\n",
    "# defina os hiper parâmetros\n",
    "taxa_ap = 0 # taxa de aprendizado\n",
    "n_iter = 0 # número de iterações de treino\n",
    "batch_size = 0 # tamanho do punhado de dados\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Nós treinamermos nossa rede neural por 2501 iterações de treino. A cada iteração, a rede neural ajustará os pesos da soma ponderada dos neurônios. A taxa de aprendizado diz se esse ajuste é grande ou não.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# converta os dados para algo que o tensorflow entenda\n",
    "x_input = 0\n",
    "\n",
    "# crie a rede neural\n",
    "deep_ann = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# treine a rede neural\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# avalie a rede neural\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
